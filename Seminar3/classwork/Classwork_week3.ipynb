{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My first neural network\n",
    "\n",
    "Today we're gonna utilize the dark magic from previous assignment to write a neural network `in pure numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from classwork_auxiliary import eval_numerical_gradient,eval_numerical_gradient_array,rel_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module template\n",
    "\n",
    "We will implement our neural network as a set of layers.\n",
    "\n",
    "Basically, you can think of a module as of a something (black box) that can process `input` data and produce `ouput` data.\n",
    "\n",
    "* affine transform: f(x) = w*x + b\n",
    "* nonlinearity: f(x) = max(0,x)\n",
    "* loss function\n",
    "\n",
    "This is like applying a function which is called `forward`: \n",
    "* `output = module.forward(input)`\n",
    "\n",
    "The module should be able to perform a __backward pass__: to differentiate the `forward` function. \n",
    "\n",
    "More, it should be able to differentiate it if is a part of chain (chain rule).\n",
    "\n",
    "The latter implies there is a gradient from previous step of a chain rule. \n",
    "\n",
    "* `gradInput = module.backward(input, gradOutput)`\n",
    "\n",
    "Below is a base class for all future modudes. _You're not required to modify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes an input object, and computes the corresponding output of the module.\n",
    "        \"\"\"\n",
    "        return self.updateOutput(input)\n",
    "\n",
    "    def backward(self,input, gradOutput):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the module, with respect to the given input.\n",
    "        \n",
    "        This includes \n",
    "         - computing a gradient w.r.t. `input` (is needed for further backprop),\n",
    "         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n",
    "        \"\"\"\n",
    "        self.updateGradInput(input, gradOutput)\n",
    "        self.accGradParameters(input, gradOutput)\n",
    "        return self.gradInput\n",
    "    \n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        \"\"\"\n",
    "        Computes the output using the current parameter set of the class and input.\n",
    "        This function returns the result which is stored in the `output` field.\n",
    "        \n",
    "        Make sure to both store the data in `output` field and return it. \n",
    "        \"\"\"\n",
    "        \n",
    "        # The easiest case:\n",
    "            \n",
    "        # self.output = input \n",
    "        # return self.output\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Computing the gradient of the module with respect to its own input. \n",
    "        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n",
    "        \n",
    "        The shape of `gradInput` is always the same as the shape of `input`.\n",
    "        \n",
    "        Make sure to both store the gradients in `gradInput` field and return it.\n",
    "        \"\"\"\n",
    "        \n",
    "        # The easiest case:\n",
    "        \n",
    "        # self.gradInput = gradOutput \n",
    "        # return self.gradInput\n",
    "        \n",
    "        pass   \n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Computing the gradient of the module with respect to its own parameters.\n",
    "        No need to override if module has no parameters (e.g. ReLU).\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def zeroGradParameters(self): \n",
    "        \"\"\"\n",
    "        Zeroes `gradParams` variable if the module has params.\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def getParameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list with its parameters. \n",
    "        If the module does not have parameters return empty list. \n",
    "        \"\"\"\n",
    "        return []\n",
    "        \n",
    "    def getGradParameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list with gradients with respect to its parameters. \n",
    "        If the module does not have parameters return empty list. \n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want \n",
    "        to have readable description. \n",
    "        \"\"\"\n",
    "        return \"Module\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear layer\n",
    "a.k.a. affine or dense layer\n",
    "\n",
    "We will now implement this layer by __filling the gaps in code__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    A module which applies a linear transformation \n",
    "    A common name is fully-connected layer, InnerProductLayer in caffe. \n",
    "    \n",
    "    The module should work with 2D input of shape (n_samples, n_feature).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Linear, self).__init__()\n",
    "       \n",
    "        \n",
    "        self.W = <initialize a random weight matrix of size (n_out,n_in).>\n",
    "        self.b = <initialize bias vector of size (n_out)>\n",
    "        \n",
    "        #here we initialize gradients with zeros. We'll accumulate them later.\n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        self.gradb = np.zeros_like(self.b)\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        \"\"\"given X input, produce output\"\"\"\n",
    "        \n",
    "        self.output = <affine transform of input using self.W and self.b. Remember to transpose W>\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        \"\"\"given input and dL/d_output, compute dL/d_input\"\"\"\n",
    "        \n",
    "        self.gradInput = <gradient of this layer w.r.t. input. You will need gradOutput and self.W>\n",
    "        \n",
    "        assert self.gradOutput.shape == input.shape,\"wrong shape\"\n",
    "        \n",
    "        return self.gradInput\n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        \"\"\"given input and dL/d_output, compute\"\"\"\n",
    "        \n",
    "        self.gradW = <compute gradient of loss w.r.t. weight matrix. You will need gradOutput and input>\n",
    "        \n",
    "        assert self.gradW.shape == self.W.shape\n",
    "        \n",
    "        self.gradb = <compute gradient of loss w.r.t. bias vector>\n",
    "        \n",
    "        assert self.gradb.shape == self.b.shape\n",
    "        \n",
    "        return self.gradW, self.gradb\n",
    "     \n",
    "        \n",
    "    def zeroGradParameters(self):\n",
    "        self.gradW.fill(0)\n",
    "        self.gradb.fill(0)\n",
    "        \n",
    "    def getParameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        return [self.gradW, self.gradb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_in, n_out = 5, 6\n",
    "\n",
    "x = np.random.randn(10, 6)\n",
    "w = np.random.randn(17, 6)\n",
    "b = np.random.randn(17)\n",
    "dout = np.random.randn(10, 17)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: Linear(n_in, n_out, w, b).updateOutput(x), x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: Linear(n_in, n_out, w, b).updateOutput(x), w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: Linear(n_in, n_out, w, b).updateOutput(x), b, dout)\n",
    "\n",
    "dx = Linear(n_in, n_out, w, b).updateGradInput(x, dout)\n",
    "dw, db = Linear(n_in, n_out, w, b).accGradParameters(x, dout)\n",
    "\n",
    "print 'Testing Linear (errors should be < 1e-6):'\n",
    "print '\\t dx error: ', rel_error(dx_num, dx)\n",
    "print '\\t dw error: ', rel_error(dw_num, dw)\n",
    "print '\\t db error: ', rel_error(db_num, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SoftMax(Module):\n",
    "    def __init__(self):\n",
    "         super(SoftMax, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        \"\"\"forward pass of softmax nonlinearity\"\"\"\n",
    "        # substract max for numerical stability\n",
    "        input = input - input.max(axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = <compute softmax forward pass>\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        \"\"\"backward pass of the same thing\"\"\"\n",
    "        exp = np.exp(np.subtract(input, input.max(axis=1, keepdims=True)))\n",
    "        denom = exp.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        e = np.diag(exp.dot(gradOutput.T))\n",
    "        self.gradInput = - np.diag(e).dot(exp)    \n",
    "        self.gradInput += exp * denom * gradOutput\n",
    "        self.gradInput /= denom**2\n",
    "        return self.gradInput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(10, 6)\n",
    "dout = np.random.randn(10, 6)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(SoftMax().updateOutput, x, dout)\n",
    "dx = SoftMax().updateGradInput(x, dout)\n",
    "\n",
    "print 'Testing SoftMax(errors should be < 1e-6):'\n",
    "print '\\t dx error: ', rel_error(dx_num, dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "You task is to implement the **ClassNLLCriterion**. It should implement [multiclass log loss](https://www.kaggle.com/wiki/MultiClassLogLoss). Nevertheless there is a sum over `y` (target) in that formula, \n",
    "remember that targets are one-hot encoded. This fact simplifies the computations a lot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from classwork_auxiliary import Criterion\n",
    "class ClassNLLCriterion(Criterion):\n",
    "    def updateOutput(self, input, target):  \n",
    "        \n",
    "       \n",
    "        self.output = <Your code goes here>\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        \n",
    "        self.gradInput = <Your code goes here>\n",
    "        \n",
    "        return self.gradInput\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(10, 6)+5\n",
    "target = np.random.randint(0, 10, x.shape[0]).reshape((-1, 1))\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: ClassNLLCriterion().updateOutput(x, target), x, verbose=False)\n",
    "dx = ClassNLLCriterion().updateGradInput(x, target)\n",
    "\n",
    "print 'Testing ClassNLLCriterion (errors should be < 1e-6):'\n",
    "print '\\t dx error: ', rel_error(dx_num, dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this example to debug your code, start with logistic regression and then test other layers. You do not need to change anything here. This code is provided for you to test the layers. Also it is easy to use this code in MNIST task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "N = 500\n",
    "\n",
    "X1 = np.random.randn(N,2) + np.array([2,2])\n",
    "X2 = np.random.randn(N,2) + np.array([-2,-2])\n",
    "\n",
    "Y = np.concatenate([np.ones(N), np.zeros(N)])[:,None]\n",
    "Y = np.hstack([Y, 1-Y])\n",
    "\n",
    "X = np.vstack([X1,X2])\n",
    "plt.scatter(X[:,0],X[:,1], c = Y[:,0], cmap='hot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a **logistic regression** for debugging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from classwork_auxiliary import Sequential,sgd_momentum\n",
    "\n",
    "\n",
    "net = Sequential()\n",
    "net.add(Linear(2, 2))\n",
    "net.add(SoftMax())\n",
    "\n",
    "criterion = ClassNLLCriterion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with batch_size = 1000 to make sure every step lowers the loss, then try stochastic version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Iptimizer params\n",
    "optimizer_config = {'learning_rate' : 1e-1, 'momentum': 0.9}\n",
    "optimizer_state = {}\n",
    "\n",
    "# Looping params\n",
    "n_epoch = 100\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch generator\n",
    "def get_batches( (X, Y) , batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "        \n",
    "    # Shuffle at the start of epoch\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_idx = indices[start:end]\n",
    "    \n",
    "        yield X[batch_idx], Y[batch_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic training loop. Examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    for x_batch, y_batch in get_batches( (X,Y) , batch_size):\n",
    "        \n",
    "        net.zeroGradParameters()\n",
    "        \n",
    "        # Forward\n",
    "        predictions = net.forward(x_batch)\n",
    "        loss = criterion.forward(predictions, y_batch)\n",
    "    \n",
    "        # Backward\n",
    "        dp = criterion.backward(predictions, y_batch)\n",
    "        net.backward(x_batch, dp)\n",
    "        \n",
    "        # Update weights\n",
    "        sgd_momentum(net.getParameters(), \n",
    "                     net.getGradParameters(), \n",
    "                     optimizer_config,\n",
    "                     optimizer_state)      \n",
    "        \n",
    "        loss_history.append(loss)\n",
    "    \n",
    "    print('Current loss: %f' % loss)    \n",
    "    \n",
    "if loss <= 0.01:\n",
    "    print (\"Well done\")\n",
    "else:\n",
    "    print (\"Something's wrong!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectified linear unit\n",
    "\n",
    "Here you will define a nonlinearity function used to build neural networks. For starters, let't build rectified linear unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "         super(ReLU, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        \n",
    "        self.output = <Your code. Please remember to use np.maximum and not np.max>\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        \n",
    "        self.gradInput = <gradient of loss w.r.t. input (passing through ReLU)>\n",
    "        \n",
    "        return self.gradInput\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(10, 6)-0.5\n",
    "dout = np.random.randn(10, 6)-0.5\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(ReLU().updateOutput, x, dout)\n",
    "dx = ReLU().updateGradInput(x, dout)\n",
    "\n",
    "print 'Testing ReLU (should be < 1e-6):'\n",
    "print '\\t dx error: ', rel_error(dx_num, dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's not try to build actuall neural network on a mnist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from classwork_auxiliary import load_dataset\n",
    "\n",
    "X_train,y_train,X_test,y_test = load_dataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build neural network\n",
    "\n",
    "By default the code below runs logistic regression. After you made sure it runs, __modify it__ to train a neural network.\n",
    "\n",
    "Evaluate how it fares with different number of hidden units.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from classwork_auxiliary import Sequential,sgd_momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = Sequential()\n",
    "net.add(Linear(28*28, 10)) #you may want to replace this guy with more layers once you got the basic setup working\n",
    "net.add(SoftMax())\n",
    "criterion = ClassNLLCriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_train_history = []\n",
    "loss_validation_history = []\n",
    "optimizer_config = {'learning_rate' : 1e-1, 'momentum': 0.9}\n",
    "optimizer_state = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epoch=40\n",
    "batch_size=1000\n",
    "learning_rate=0.001 #try decreasing over time\n",
    "\n",
    "for i in range(n_epoch):\n",
    "        \n",
    "    for x_batch, y_batch in get_batches((X_train, y_train ), batch_size):\n",
    "        net.zeroGradParameters()\n",
    "\n",
    "        predictions = net.forward(x_batch)\n",
    "        loss_train = criterion.forward(predictions, y_batch)\n",
    "        loss_train_history.append(loss_train)\n",
    "\n",
    "        dp = criterion.backward(predictions, y_batch)\n",
    "        net.backward(x_batch, dp)\n",
    "\n",
    "        sgd_momentum(net.getParameters(), net.getGradParameters(), optimizer_config, optimizer_state)      \n",
    "\n",
    "        test_idx = np.random.randint(0, X_test.shape[0], batch_size)\n",
    "        loss_test = criterion.forward(net.forward(X_test[test_idx]),y_test[test_idx])\n",
    "        loss_validation_history.append(loss_test)\n",
    "\n",
    "    print('epoch %s: rate = %f, loss_train = %f, loss_test = %f' % (i, learning_rate, loss_train, loss_test))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once you got it working\n",
    "\n",
    "Go to HW3_* notebook. The homework starts there.\n",
    "\n",
    "\n",
    "You're welcome to re-use code for modules (liner, softmax, relu and loss) in the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
